FINISH
* 1 2 
* 3 4 5
* 6 7 8 9
* 10 11 12 13 14 15
* 16 17 18 19 
* 20 21
* 22 23 24
Lec 1
Database Query
* Basic query
    * Where -> select(filter)
    * Select -> project
    * Union 
    * intersect
    * except
    * Join -> foreign key combine
* Advanced query
    * Aggregate  select avg(gpa) from ..
    * Group by.   Select avg(s.gpa), e.cid from enrolled as e, student as s where … group by e.cid
        * 1. Where filter 2. Group by 3. aggregate
    * Having  filter results based on aggregation computation
    * String function: %, _ 
    * Output redirection:
        *  Create table xxx( )
    * Output control
        * Order by 
        * limit
    * Nested query
        * Select name from student where Sid in ( select cid from enrolled)
        * Select sid, name from student where Sid => all (select Sid from enrolled)
    * Window function ( not familiar)
    * cte = nested query but can be used as a recursion
Lec 3
Disk manager -> disk-oriented -> we move data back and forth on disk and storage
* overview
    * Sequential access is faster than random access
    * It is like virtual memory!! -> not use OS, when we fetch data, we still want something running at the same time!
    * Database system better -> it has a lot of information os doesn’t know
        * Scheduling better
* Two things
    * How DBMS represents data on disk.   -> storage manager
    * How DBMS move data back and forth
How the DBMS represents the Database on disk?
* Storage manager
    * Track read and write
    * Track the available space
* DBMS stores a database as files on disk
    * File -> a lot of pages (fixed size) -> make things easier when you have a fixed size page
        * ( contain data + meta-data, log, indexes) ( Storage manager responsibility)
        * Every page has its id ( map the page to the physical location)
        * One page for specific type data
            * Single page save dic + data
* Heap
    * definition
        * Collection of pages
        * Can get/delete pages
        * Can Iterate pages
        * Two ways to know what pages exist and which one has free space
    * Linked list implementation:
        * Free page pointer(you can store data) + data page pointer(no free space)
        * Each page keeps track of the free slots
        * We have to go along the pages to insert some data or find the specific pages
    * Page directory
        * Maintain a page that tracks the location of the data pages
        * Also records the free slots of each page
        * You know every page location, don’t need to look all the pages
            * Some cases
                * Flush out every directory( which has free space, which does not have free space)
                * We can reconstruct the page if the directory page is trashed
* Page Layout ( how to organize the data in the pages )
    * Slotted pages
        * Header + data
            * Header: the # of used slots + The offset of the starting location of the last slot used.(slot array)
            * Data: tuples
    *  Log structured -> read is slower ( it has go back and reconstruct the data)
        * Only store the log of how the db is modified
        * Log compunction ( smaller to larger logs to remove unnecessary records)
            * Level , universal 
* Tuple layout （sequence of bytes)
    * The job of DBMS is to interpret the bytes into attribute types and values
    * Interpret bytes ->  attribute types + values
    * Tuple
        * Header (meta-data) (null? Concurrency control) + tuple data ( the struct is defined when you create a table)
        * Pre-join -> denormalize 
* Record IDs ( keep track of where the tuples are ) -> keep track of the individual tuple
    * Record ID = page_id + offset/slot
Lec4
* Disk-oriented architecture
    * Primary location of database is on non-volatile disk
* Database system
    * On disk
    * Buffer pool -> 1. Directory 2. Page2
    * Execution engine 
* Overview
    * How to represent individual values
    * System catalogs ( meta data for how to interpret the bytes )
    * Storage models ( take data in tables and map them in pages )
Lec 5
* How the DBMS move data back and forth
    * Spatial Control -> where to write pages on disk
    * Temporal Control -> when to read pages and when to write them on disks
        * To minimize the stalls (back and forth) 
* Overview
    * Buffer Pool
        * Frame = slots in tuples
        * When query, we have a copy in buffer pool
    * Buffer pool meta data -> page table
        * Page table -> currently in memory pages
        * When read, pin on the page, and do not allow to evict it. -> replace?
        * Dirty flag -> some writing happens -> just remove it or write back !!
        * When read a page that is not there, (a latch) , then find the page and copy it.
    * Locks and latches
        * Latches -> protect data structures (  implemented level  ) 
        * Locks -> protect database entities ( high level ) 
    * Page table vs page directory
        * Dir : Page ids -> page locations -> store on disk
        * Table : page ids -> buffer pool locations -> in memory store
        * Multiple buffer pools -> hashing to different buffer pools
    * optimization
        * Pre-fetching
            * When retrieving, it will also retrieving others ( based on the query plan, we know what we are doing )
        * Scan sharing
            * Rather start from scratch, scan with the one who is doing the job you are doing.
* Buffer Pool replacement policies
    * Goals
        * Correctness ( do not make update page throughed away )
        * Accuracy( do not throw away pages that are in need )
        * Speed ( it will hold a latch when we retrieve page ) 
        * Meta-data overhead
    * Policies
        * LRU 
            * Maintain a timestamp when accessed
            * Select the one with oldest timestamp when evicted.
        * Clock
        * To be done
Lec 6 Index  -> access methods
* Overview
    * Data structures
        * Internal meta-data
        * Core data storage -> store the tuple as value
        * Temporary -> create a hash table and then through away
        * Index -> store pointer as value
    * Design decisions
        * Data organization -> how we lay out our data for efficient access
        * Concurrency -> how to enable threads to access the data structure at the same Time without problem.
    * Map key to value
        * Key -> Hash function ->lands in a key  in an offset in the array(O(1)) -> point to the value
* Static hash table -> assumption perfect hash function + unique key
    * Allocate each key to an array location
    * No collision -> you can not know the key number
* Hash function (how to map key to a smaller domain ) + hash scheme ( handle collision )
* Properties of different hash ( fast + low collision rate )
    * Hash functions
    * Hashing schemes
        * Linear probe -> must have key|value in each location ( it may have some key in same location, we have to determine whether it is the same key ) 
            * Some cases
                * Delete 
                * Handle not unique key
                    * Store separate storage area for each key
                    * Store keys entries together 
            * In collision -> hash table becomes O(n) for maintaining the hash table
        * Robin-Hood 
            * If come across the key, if it is nearer, then we take its place. 
            * It holds key|value|position_diff
        * Cuckoo hashing 
* Don’t know the key numbers in advance 
* Dynamic hash table
    * Chained hashing
    * Extenible hashing
    * Linear hashing 
Lec 8
Index 
    * Overview
        * Table indexes -> replica of subset of table’s columns that are organized for efficient access using the subset 
            * Must keep the content of table and content of index synced ( logically )
        * It is database system’job to figure out best index for use to execute query -> statistics 
    * B+ tree ( real keys and values are stored in the leaf nodes )
        * M-way search tree  +  balanced  
        * Sibling pointer! 
        * <value> / <key> : key -> columns  value: inner node -> pointer to nodes, leaf node : record id ( page id + slot offset )  / record content ( no heap )
        * Array is sorted in key ( can do binary search ) 
    * B+ tree operations 
        * Unique index
            * Insert
            * Delete 
        * Non-unique index
            * Duplicate keys
                * 
            * Value list
                * 
        * Variable length key
            * Store the pointer for a tuple’s attribute (no)
            * Variable length nodes
        * Prefix compression
            * Prefix compression
                * Only store the minimum prefix for route
        * B+ tree is actually doing
            * A linked list with lot of nodes to help it to jump over -> another implementation skip lists
            * 
    Lec 9
*     How the DB build these keys
    * Automatically create an index for enforcing the integrity constraints
    * Forign key constraint
        * Use index to find a match ( my sql must have a unique index or it will not has a foreign key constraint )
Lec 10
* Concurrency control
    * Ensure the concurrent operations on a shared object
* Locks and latches
    * Locks -> protect the logical contents of the db
        * Hold the whole duration of the transaction 
        * Roll back 
    * Latches -> protect the internal data structures
        * Hold for operation duration
* Latch modes
    * Read mode
        * Multiple threads allowed
    * Write mode 
        * Only one thread
        * Can not acquire if another thread is in any mode
* problems
    * Modify same Time -> write latch , done , realease
    * Traversing -> latch crabbing/ coupling
        * Multiple threads to access b+ trees at the same time
            * Get latch for parent
            * Get latch for child 
            * Release latch for parent if “safe"
        * Read 
        * Insert 
        * Delete 
* Better solution 
    * Latch first 
        * Assume that you will not do a split or merge
            * If you are wrong -> go back and restart 
            * Not wrong -> happy
Lec 11 
* Query plan = trees
    * SQL -> Operator ->
        * 
* Process Models -> How to execute a query plan
    * Iterator model ->
        * Implements next -> give you a tuple (null for no)
        * Top down -> parent need tuple -> child need tuple -> leaf give you the tuple
    * Materialization model -> transactional 
        * Implement using a output buffer 
        * Bottom up -> bottom buffer -> upper level buffer 
    * Vectorization models
* Access Methods -> How DBMS access the data stored in the table
    * Sequential scan 
        * Maintains a cursor -> the last tuple I have seen
        * 
        * Optimization 
            * Zone map
            * Late materialization
            * Heap clustering
    * Index scan -> knows what page we are going to read 
        * Pick the most selective index -> if there is an index, do the index filter first
    * Multi-index scan 
        * Do two filter, then combine them together using intersection ( and ) or union ( or )
        * Optimization 
            * Index scan page sorting -> don’t fetch, sort the page id then ,fetch 
            * The relational model -> the order does not matter  !!!!
* Expression Evaluation
    * Where clause is a expression tree
    * Maintain a execution context  ->  for each tuple figure out it is true or not 
    * 
Lec 12 
* Overview
    * Why we need join -> normalize table 
        *  no redundant, we split the table to different table -> so they have no repeat 
        * Join, put the split tables together -> reconstruct the tables
        * 
    * Joining two tables
* Joining output
    * Copy 
    * Record ids -> late materialization 
* Join algorithm
    * Nested loop
        * It sucks because we have to do a sequential scan to get a match
        * Naive 
            * 
            * Calculate the IOs for the cost 
            * Use the smaller in the outer loop
            * Don’t need to get each tuple using one page
        * Block nested
    * Index 
        * Use an index to find. A inner table match.
        * 
        * Basic idea 
            *  put smaller table as outer table
            *  buffer as much outer table
            * Loop over inner table or use an index
    * Sort-merge join
        * process
            * Sort join keys
            * Walk two cursor and find match
        * High level idea
            * Because the previous tuples need not to be relooked 
    * Hash join
        * Hash will get the same value for same key
        * process
            * Build hash function on join (outer)
            * Use the hash function -> get to the outer tuple
            * 
    * Grace Hash join 

Lec 13
* Overview
    * SQL -> what you want 
    * Query plan -> how we actually do this.
        * Query optimizer = compiler -> choose the best algorithm based on what it knows about the data 
* Rule
    * Rewrite the query 
* Cost-based Search
    * Use a cost model and pick the lowest cost one
* An overview
    * Binder will lookup the system catalog and get the internal metadata of the DB
        * Name -> Internal ID
    * Apply rules to rewrite 
* Relational Algebra equivalences
    * Rule engine check the rule
    * example
        * WHERE
            * Predication pushdown -> do the filter first
            * Reordering the predicates
            * Simplify the predicates
        * PROJECTIONS
            * Push projections down 
                * Copy less data from one operator to the next 
        * Example
            * Impossible predicates or unnecessary ( 1=0 , 0=0 ) 
            * Join elimination -> join just happens in unique key or primary key, other cases do not make sense!
            * Ignoring projections 
            * Merging predicates
            * Joins -> reordering!! -> must prune the search space
* Plan cost estimation -> how much disc reading 
    * How many tuples will be read/write -> based on the statistics  ( database catalog )
    * Statistics 
        * Number of relation.          N(R)
        * Number of distinct values V(A,R)
        * Selection cardinality          SC(A,R) = N / V -> assume data uniformity
        * Selectivity    SC/V ( a= xxx 有多少tuples /  不限制下有多少tuples ）（ range query works the same ) -> probability
        * assume predicates are independent 
    * Use the selectivity to handle
    * Sampling 
* Plan enumeration -> build query plan
* Nested sub-queries

Lec 14
* Parallel execution
* Parallel vs distributed
    * Parrallel 
        * Nodes close to each other
        * Communication is small
    * distributed
        * Nodes far
        * Use public network
* parallelism
    * Inter-query -> hard for updating
        * Different query same time
    * Intra-query -> two approaches
        * Single query in parallel
            * Horizontal -> different instances to run different subsets of data
            * Intra 
                * Use exchange operator to combine the result 
                * Break up to different threads to execute it
            * 
        * Inter 
            * Piplined
            * 
            * Two task must in the same pace
    * Access data must be parallized
        * Multi-disc parallelism
            * Configure OS to store files across multiple storage devices
                * Storage appliance 
                * RAID
        * Database Partitioning 
                * Split single table into disjoint physical segments 
                    *  column store
                    * Sharding -> based on partitioning key -> go to different partition
* Process model
    * Worker -> responsible for executing tasks
        * Process per worker -> one query one worker
            * Dispatch -> worker 
            * Single process for one worker -> OS scheduler 
            * No threads library 
        * Process pool -> one query multiple worker  
            * Hand things to worker that is free
        * Thread per worker
            * Single process with multiple threads
            * Better 
                * Less overhead per context switch
* Scheduling 
Lec 15
* Embedded logic
    * Conversational database API
        * 
        * Only one connection one time -> block between two SQL
    * Embedded logic
        * Move application logic to SQL 
        * Efficiency , reuse
    * Some logic inside DBMS
        * UDF
            * SQL + programming language 
            * Read only
        * Stored procedures
            * Input, output 
            * Modify tables
        * Triggers
            * Some event -> UDFs
        * Change notification
            * Pub/sub notification
    * Not built-in types
        * Attribute splitting 
        * Application serialization -> JSON 
            * Have to deserialize it in application code
    * User defined type
        * Expose a user defined API to extend the database system
    * views
        * Virtual table
            * To application, it is a table -> used to hide attributes
            * Internally, your query will be rewrite to do actually what they do. 
                * Always rewrite query 
            * Materialized only when needed -> so it is dynamic 
        * SELECT INTO -> create a new real table, which is static
    * Materialized view
Lec 16 -> concurrency control
* Motivation 
    * Both change the same record in table -> concurrency 
    * Transfer 100, power failure -> durability
* Transaction 
    * ACID guarantee 
    * A sequence of operations on a shared database to perform high-level function (application level)
    * It is the basic unit of change in DBMS
* Example 
    * 
    * It is actually run with other transaction in the same time, but it should not see the effects of other transactions.
    * After you type commit, if all the actions are done, you do the I/O and send ack.
                    * If not done, then wait, when it truly done, it will do the I/O, then send ack.?
    * DML is with implicit commit -> update … = begin update …. Commit 
* Strawman system -> sqlite
    * A pointer, a queue
    * Copy 
    * If the transaction is complete -> pointer move to the new file
* Allow concurrent execution of independent transactions
    * Throughput
    * Repsonse time
    * Interleaving but still correct
    * Balance approach
* Problem -> we must define what interleaving transactions are correct
    * Temporary inconsistence ( unavoidable )
    * Permanent inconsistence ( bad )
* Transactions scope 
    * Only care about the data, read/write
    *  
        * Commit -> all change saved, can not roll back
            * When you say commit, it is not truly commit, when the DBMS told you that your change committed, the transaction is committed.
        * Abort -> undo all the change this transaction do -> can be activated by the DBMS ( maybe because of integrity constraint ) ( like there is nothing happens ) 
* Correctness criteria: ACID
    * Atom: happen or not happen
    * Consistency: for distributed, 
    * Isolation: an illusion that it only execute itself  -> the transaction begin in the 13s, then it run as if it runs alone in 13s ( even though there will be interleaving )
        * Use concurrency protocol to ensure isolation
    * Durability : after commit, machine crash, it is still there.
    * 
* A
    * outcomes
        * Commit it 
        * Abort 
    * Grantees atomic
        * All execute or no actions at all
    * How to do it 
        * Logging
            * Log all the actions, if there is something wrong, go back!
        * Shadow paging
            * Copy the pages that transactions is made on.
* C
    * The world is represented by the database -> modeling the world  -> high level
        * Database consistency -> models the world correct, follows integrity constraints. -> single node changes, other node also changes
        * Transaction consistency -> based on the application code you write
* I -> when one txn stalls because of resources, another txn continues.
    * As if it is the only user
        * How to achieve this
            * Two protocals
                * Pessimistic: don’t let problem arise in the first place -> like crabing 
                    * Ask for permission before do something
                * Optimistic: assume conflicts are rare, do, but if bad happens , undo -> 
            * Example
                * 2000 -> 2120 consistency still model the real world
            * When one is stalled, other one can continue doing what they can do if there is no conflict. 
            *  
            *  DBMS only sees read and write
        * It is correct if it is the same with a serial execution schedule. -> dbms only sees R,W
            * Serial schedule -> no interleaving
            * Any state in the DBMS is the same with the schedule -> it is same
        * Properties of schedules
            * It will not ensure that when you issue an transaction, it will run in order.
        * It is serializable if it is same with the serial schedule.
            * 
        * Serilizeable schedule -> 
        * Txn issues first does not mean that it must execute first -> the flexibility allows db to choose interleave the transaction 
        * Also when you commit , it is not truly commit. When it commits, it will send ack to tell you that it commits.
    * Conflicting operations -> start to know what exactly is the equivalence.
        * Different txns
        * Same object, and one is write
        * 
    * basic rule 你不能让别人写你需要读写的东西，你写了的话别人也不能读你写的东西。
        * 所有东西应该在你commit之后才可以进行.
    * R,W -> 
        * Unrepeatable read -> read twice, read the internal in other transactions.
        * 如果读了，别人不能写，因为这样下次读的时候你会读到别人写的东西。
        * 
    * W,R -> expose the inconsistent state of the DBMS to outside of the world.
    * 如果写了，别人不能读，因为你读到的是中间状态，还没commit（随时可能roll back)
        * Write into the buffer pool does not mean that we get the durability of the data. The buffer pool data may be inconsistent state.
        * it will cause cascading abort
            * One is aborted, and another must abort too.
        * 
        * T2 read dirty data from A ( not committed data )
    * R,R -> change the uncommitted data
    * 如果写了，别人不能再写，因为这样你自己写的数据就没了..
        * 
        * 19, Bieber 
    * Cascading aborts
    * Determine if it is serializable 
        * Conflict equivalent -> every conflicting actions are ordered in the same way -> if it is equal to 
        * When you have the time for each transaction
        * What is conflict serilizable
            * Move actions up and down.-> there is one that is equal to the serial transaction 
            * If it is not conflict, we can reorder it 
            * Will wait until the lock is released.
    * Dependency graph
        * If tj wants to use ti’s resource, then you have an edge ti -> tj
        * No cycle 
        * Inconsistent analysis 
    * View serializable 
        * Rewrite some application code so that it will be correct even if it is not conflict serializabl
        *  -> application correctness, but not seen by database
            * We only care about that we read A, and get A finally updated by T3
        * No one can do this 
    * Schedules 
        * 
* D
    * Make sure that all the transactions that are committed, it is really committed -> you told them that it is committed, it is truly committed.
    * Buffer pool is not persistent, so we have to log everything before real commit.
* Should be done by smarter people to do this and get an abstraction of it.


Lec 17 
* Last class
    * Give you a schedule -> it is serilaizble or not
* What we need to do
    * Guarantee that all execution schedules are correct without knowing the entire schedule ahead of time.
        * Solution, to use locks to protect. -> pessimistic protocol 
    * Lock manager -> traffic cop
* Lock types
    * Basic
        * S-lock
        * X-lock
        * 
        * if I crash,  memory got wiped, no transactions -> no need to maintain the lock table anymore.
    * Problem of S-lock and X-lock
* Two-phase lock -> do the transaction managing on the fly -> generate schedule that is serializable
    * Two phases -> must follow 
        * Growing 
            * Request locks
            * Lock manager grants or denies
        * Shrinking 
            * Only release locks
    * Cascading aborts -> dirty read
        * Strict 2PL
            * Strict means that a value written by a txn is not read or overwritten by other txns until that txn is finished. -> the serialized order 
            * Just before the commit, it release all the locks
                * No Cascading abort 
* Two-phase locking
* Deadlock detection + prevention 
    * 
        * Deadlock detection 
            * Wait for graph
            * 1s get the graph and decide how to break it  -> a parameter to set up
            * Some decision
                * Select a victim to rollback to break the cycle
                    * Choose victim
                        * By age
                        * Different 
                *  Rollback length
                    * completely
                        *  
                    * Minimally
                        * 
            * 
        * Deadlock prevention
            * Wait-die 
                * Old one will be waiting the young txn 
                * High -> wait
                * Low -> abort
            * Wound-wait
                * High -> kill the txn
                * Low -> wait
                * 
        * Only one can be used
        * It will maintain the original timestamp
* Hierarchical locking -> it minimizes the work when you want to acquire or release the locks                   
    * Acquire lock -> decision on what it is on 
    * When you hold a lock on database, you hold locks on everything below
    * Intention lock
        * Hints for upper part -> you can get the locks or not -> do not need to check below. -> you may not get the 
* Phantom problem
    * Can lock on the tuple that does not exist 
    * Predicate locking -> no 
        * If it satisfies the predicate, then it will not be permitted 
    * Index locking
        * Find the index and take a lock on the node， find all the entries, take a lock on the index.
    * Locking without an index -> no
        * Lock on each page
        * Lock on the table ( no delete or insert ) 
    * Repeating scans
        * Re-execute every scan when commits , if there is difference, then roll back.
* 
* Isolation levels
    *  Controls the extent that a txn is exposed to actions of other concurrent tens.
    * 
    * 
        * Depends on the isolation and the txns
    * 
* Access mode 
    * Default -> read write
    * You can change it to read only -> no need to lock
Lec 19
* Overview
    * Multi-version concurrent control - MVCC
        * DBMS maintains multiple physical versions of a single logical object in the database.
    * They will read a snapshot and will not get the internal state.
    * Time-travel queries -> fanaicial 
* How it works
    * Version -> was this visible to a txn 
        * Read -> is this tuple visible to me ? Read 
        * Write -> create new version and set end time.
        * Aborts? Undo the all timestamps -> txn status table. 
            * Active -> not committed yet. Does not exist before.
        * Commit -> the txn table is done.
    * You will get different tuples inside the database -> with different time version
    * Don’t know wtf is this!
        * Don’t know the concurrency control protocol throughly.

Lec 22 -> distributed system
* Overview
    * What does it include
        * Nodes are far
        * Nodes are connected by public network
    *  Still has to cover the part we include in the parallel DBMS
        * We can not assume that our machines are reliable.
        * 
        * OLTP -> amazon, update a little piece of data.
        * OLAP -> read only, see the all data
* System architectures
    * How to coordinate with each other.
        * 4 different   
            * Single node -> I only know how to get data from my own disk.
            * Memory is not local -> can be got by lots of CPU
            * Cpu has its own memory (different )->  disk is shared by a lot of memory and CPU. Everything in memory is just local to itself.
            * Share nothing -> it just use the network to communicate with each other. ( here I change something )
    * Shared memory 
        * Each processor has a global view of all in-memory data structures
        * OS level
        * Must know others exist ( communicating by message)
    * Shared disk
        * Cpu has its own memory 
        * Different process to same disk
        * Anything related to the memory has to be passed through network
        * Stateless -> can scale up easily
    * Shared nothing
        * Easy to increase capacity
        * Hard to ensure consistency
        *  
        * Each time you add a node, you shuffle the data.
* Design issues
    * How does application find data
    * How to execute queries on distributed data
    * Ensure correctness
    * High level
        * Homogenous nodes
            * One node can do anything
            * Shared nothing architecture
        * Heterogenous nodes
            * Different nodes diffrent tasks 
            * Shared disk
            *  
            * You can scale up each of these 
    * Data transparency 
        * Don’t need to know the physical location of each data
        * Data partitioning
            * naive way
                * 
                * Join -> no use
            * 
* Partitioning schemes
    * Sharding -> excites query fragments on each partition and combines the result. 
    * Naive partitioning 
        * Each table go to one partition 
            * Easy to know which partition need to go.
            * If you don’t need to read, put it in one.
            * Problem -> not scalable 
                * Hot spot
                * Joins 
        * Horizontal partitioning 
            * Just partition one time
            * Consistent hash
            * 
        * Logical partitioning
            * Update will go to the single node
        * Physical partitioning
            * Really go to different disks
* Single vs distributed -> make that all txn to one node. -> distributed txn is hard -> must need coordinate
    * Single -> no coordinate
    * Distributed -> requires coordination
* Transaction coordination
    * Centralized -> global cop
        * TP monitors 
            * Lock request -> ack
            * Commit request -> 
                * partitions -> safe?(constraint violation?)
                * ack
        * Middleware
            * Query request -> know where it stores and locks state ( know where you should go to )  -> send to the partitions
            * Commit request -> safe? -> ack
    * Decentralized -> nodes organize themselves
        * Pick a node -> responsible for the txns 
            * Ack to where to go
            * Commit -> communicating with other nodes. ( safe commit )
        * Commit request -> safe to commit? 
* Distributed concurrency control 
    * Hard ->
        * replication
        * Network communication
        * Node failures
        * Clock knew
    * Have to know their lock states.
Lec 23
* Problem
    * A node fails 
    * Show up late 
    * Don’t wait for agree
* Atomic Commit Protocols ( consens protocal )
    * Two-phase commit
        * Prepare 
            * Commit to base (coordinator) 
            * Sends to participant to know if it is involved
            * Logging all the message of sending and ack.
        *  
        * Commit 
            * Committed -> send back success
        * Abort
            * Sees abort -> abort the transaction. 
        * Early prepare voting 
            * Last query, it will piggyback the ack and result. Don’t need to .
        * Early ack after prepare
            * When gets ok, you send back success. ( do not need )
                * Log all the message, if crash, you still know what to do.
    * Coordinator crash -> abort
    * Participant crash -> think as it sends abort 
* Paxos -> majority 
    * Minority figure out how to commit the transactions.
    * 
* Replication
* Consistency Issues
* Federated Databases


Donald DB
* Resource Model
    * Collection = Table
    * Resource = Row
    * Sub_R = weak entity
    * Sub_Collection = ?
* Weak entity: 
    * Order contains Order details 
    * When one order is deleted, all the order details is deleted.
* Application Layer:
    * RDB DataTable


Lec 1
Nothing 
Lec 2 
Lec 6
* Triggers
    * Update, delete, insert
* Inheritance 
* 1-2-3 table
Lec 7
* hw3
    * Engines
        * Schema 
        * Query
        * Durability
        * Control Access -> use isolation ( wait illusion that they are using the database)
    * Architecture
        * Query compiler -> SQL -> program -> execute engine -> run
        * (Find) Index: finding and locating the row
        * (Access) Buffer -> memory to hold rows 
        * (Load/save) Storage 
        * transaction manager : AICD  make sure makes AICD behavior (isolation)
    * Disck
        * Delay
            * Move the head to the track
            * Wait the Sector
            * transfer
        * Sector + track -> 
            * Elevator algorithm -> seek time
        * RAID
            * Several disks into one disk
            * RAID0 -> fast, like one disk RAID1 -> in case of failure
            * RAID 1+0
            * RAID5 -> correction block (parity block) -> but write slower ( need to write two)
    * Record block
    * Index
        * Overview 
            * Key = put index on it: if the key is (foo), go to row (bar)
            * Hash 
            * b+ tree (ordering is important)
        * Some terms
            * Cluster , uncluster -> same order or not
            * Dense -> one index for one entry, Sparse -> one index for one block
Let 8
* Index 
    * Lucent/solr
    * 
    * Ingestion -> parse it and 
    * https://www.geeksforgeeks.org/inverted-index/
* DBMS vs OS
    * Has to pin a page, and has a replacement policy
    * File system -> 
        * Block I/O 
        * Bypass the operating system -> operate the block I/O
* Keep track of pages, free space, records
    * Heap file 
    * 
* Query processing
    * Parser/translator -> tree
    * Optimizer 
    * Engine
* Some common
    * Push select 
    * Push where
    * Access path selection ->
        *  -> choose the most selective index ( HW3 ) -> also apply in the join
*  Also in the hw3
* Statistics -> update -> you have the statistics of the  tables so that you can do the optimization based on that.

Recovering 
* Know the logs
    * Redo 
    * undo 
书
Relational Model
* Relational model
    * Table -> relation = set of tuples(rows)
        * Set of rows -> relation instance 
    * Row -> tuple
    * column -> attribute
* Schema
    * The value and the attribute of each relation
* Keys
    * A way to distinguish tuples in a given relation
    * Superkey, candidate key(min super key) , primary key（chosen candidate key)
    * The values of keys can uniquely define a single tuple.
    * Foreign key
        * One relation has primary keys of other relations (r1 -> r2)
            * R1 foreign keys (referencing) r2 (referenced)
* Operations
    * Selection:
        * Return rows that satisfy predicate
    * Projection
        * Output specified attributes
    * Natural join
        * Pairs of rows(same attributes-> same values)
    * Cartesian product
        * All pairs of rows
    * Union 
        * Combine two relations
* SQL
    * DDL
        * Create table , primary key, foreign key
    * SQL query
        * Select 
            * Select distinct 
            * + - select salary * 1.1
        * Where ( predicate )
            * select name from instructor
            * from instructor
            * Where name = ’Comp. Sci.’ and salary > 70000;
        * Multiple Relations Query
            * Select -> attributes desired in result of query 3
            * From -> relations to be accessed 1
            * Where -> predicate 2
        * Natural join
            * Only consider the tuples with same names in attributes and concat together
            * * join on 什么表明他们是对什么信息的合成.
                *     * Dept_name + id 对于同一学院 同一课程 ( define the set )  (dept_course set) -> other attributes are the data you want 
                *     * id 对于课程 (course set)
            * * Rename
                *     * Attribute-name, relation-name
                *     * Join with itself
            * * String function
                *     * Intro%
                *     * %comp%
                *     * ___
                *     * ___%
            * * Order 
                *     * Multiple attributes
                *         * Order by salary desc, name asc
            * * Where 
                *     * Where -> 
                * * Set operations
                * * Null values
            * * Aggregate function
                *     * The not group by attribute must be aggregate function
            * * The meaning of aggregation function
                *     * 1. From to get a relation
                *     * 2. Where Predicate is applied
                *     * 3. Group by to get tuples into groups, if no group by, then all is one
                *     * 4. Applied to each group
                *     * 5. Generate results based on the tuples.
            * * Sub Query
                *     * A select-from-where expression
                *     * Where example
                *         * Set membership
                *         * Set comparison
                *         * Correlated subquery
            *     * Where example
                *         *  Because all select from where all return relations
            *     * Scalar subquery
                *         * Only return a relationship with one single attribute
                *         * 相当于把一个加上去
    * 
            * 

问题：
* Is order really does not matter?
